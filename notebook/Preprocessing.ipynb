{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Preprocesamiento de Datos - Retail Sales Dataset\n",
        "## Felipe Lucciano Santino Di Vanni Valenzuela\n",
        "### Pipeline de Machine Learning - Etapa 2/3\n",
        "\n",
        "---\n",
        "\n",
        "## **Objetivos del Preprocesamiento**\n",
        "\n",
        "Este notebook es la segunda etapa del pipeline de machine learning:\n",
        "\n",
        "1. **EDA.ipynb** COMPLETADO: Análisis exploratorio profundo\n",
        "2. **Preprocessing.ipynb** (Este notebook): Preparación de datos para ML\n",
        "3. **Benchmarking.ipynb**: Implementación y comparación de modelos\n",
        "\n",
        "### **Objetivos Específicos**\n",
        "\n",
        "- **Carga de Datos**: Importar resultados del EDA\n",
        "- **Transformación de Variables**: Encoding, escalado, feature engineering\n",
        "- **Preparación para ML**: División train/test, creación de pipelines\n",
        "- **Validación de Calidad**: Verificación de transformaciones\n",
        "- **Exportación**: Datos listos para modelado\n",
        "\n",
        "---\n",
        "\n",
        "## **Transformaciones Aplicadas**\n",
        "\n",
        "1. **Encoding de Variables Categóricas**: Label encoding y one-hot encoding\n",
        "2. **Escalado de Features**: StandardScaler para variables numéricas\n",
        "3. **Feature Engineering**: Creación de variables derivadas\n",
        "4. **Tratamiento de Outliers**: Manejo basado en insights del EDA\n",
        "5. **Creación de Variable Target**: Definición del problema de clasificación\n",
        "6. **División de Datos**: Train/validation/test splits estratificados\n",
        "\n",
        "---\n",
        "\n",
        "## **Outputs del Preprocessing**\n",
        "\n",
        "Al finalizar este notebook tendremos:\n",
        "- Datasets preparados para entrenamiento y evaluación\n",
        "- Transformadores ajustados para aplicar a nuevos datos\n",
        "- Documentación de todas las transformaciones aplicadas\n",
        "- Validación de la calidad de los datos procesados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## **1. Configuración del Entorno de Preprocesamiento**\n",
        "\n",
        "### **Librerías para Machine Learning**\n",
        "\n",
        "Importamos las librerías especializadas para preprocesamiento y machine learning:\n",
        "\n",
        "- **scikit-learn**: Transformadores, escaladores, y herramientas de ML\n",
        "- **pandas/numpy**: Manipulación de datos y operaciones matemáticas\n",
        "- **pickle**: Serialización de objetos y transformadores\n",
        "- **joblib**: Persistencia optimizada para objetos de sklearn\n",
        "\n",
        "### **Configuración de Reproducibilidad**\n",
        "\n",
        "Establecemos semillas aleatorias para garantizar resultados reproducibles en todas las transformaciones que involucren aleatoriedad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entorno de preprocesamiento configurado correctamente\n",
            "Librerías de scikit-learn importadas\n",
            "Semilla aleatoria establecida para reproducibilidad\n"
          ]
        }
      ],
      "source": [
        "# Importación de librerías para preprocesamiento de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "# Librerías de scikit-learn para preprocesamiento\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Configuración de reproducibilidad\n",
        "np.random.seed(42)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de pandas para mejor visualización\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"Entorno de preprocesamiento configurado correctamente\")\n",
        "print(\"Librerías de scikit-learn importadas\")\n",
        "print(\"Semilla aleatoria establecida para reproducibilidad\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### **Carga de Datos del EDA**\n",
        "\n",
        "Cargamos los datos procesados en la etapa de EDA y el resumen de hallazgos para informar nuestras decisiones de preprocesamiento.\n",
        "\n",
        "**Archivos de Entrada:**\n",
        "- `retail_eda_processed.csv`: Dataset con transformaciones básicas del EDA\n",
        "- `eda_summary.pkl`: Resumen de hallazgos y recomendaciones\n",
        "\n",
        "**Verificaciones:**\n",
        "- Integridad de los datos cargados\n",
        "- Consistencia con hallazgos del EDA\n",
        "- Validación de variables creadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CARGANDO DATOS DEL EDA...\n",
            "==================================================\n",
            "COMPLETADO: Dataset cargado exitosamente: (1000, 16)\n",
            "COMPLETADO: Resumen de EDA cargado exitosamente\n",
            "\n",
            "VERIFICACIÓN DE INTEGRIDAD:\n",
            "----------------------------------------\n",
            "• Dimensiones del dataset: (1000, 16)\n",
            "• Período de datos: 2023-01-01 a 2024-01-01\n",
            "• Valores faltantes: 0\n",
            "• Clientes únicos: 1000\n",
            "• Variables del EDA presentes: 6/6\n",
            "COMPLETADO: Todas las variables del EDA están presentes\n",
            "\n",
            "INFORMACIÓN DEL DATASET:\n",
            "----------------------------------------\n",
            "Columnas disponibles: ['transaction_id', 'date', 'customer_id', 'gender', 'age', 'product_category', 'quantity', 'price_per_unit', 'total_amount', 'year', 'month', 'day', 'dayOfWeek', 'quarter', 'weekOfYear', 'rangoEtario']\n",
            "\n",
            "Primeras 3 filas:\n",
            "   transaction_id       date customer_id  gender  age product_category  quantity  price_per_unit  total_amount  year  month  day  dayOfWeek  quarter  weekOfYear   rangoEtario\n",
            "0               1 2023-11-24     CUST001    Male   34           Beauty         3              50           150  2023     11   24          4        4          47  Joven Adulto\n",
            "1               2 2023-02-27     CUST002  Female   26         Clothing         2             500          1000  2023      2   27          0        1           9  Joven Adulto\n",
            "2               3 2023-01-13     CUST003    Male   50      Electronics         1              30            30  2023      1   13          4        1           2        Adulto\n"
          ]
        }
      ],
      "source": [
        "# Carga de datos del EDA y resumen de hallazgos\n",
        "\n",
        "print(\"CARGANDO DATOS DEL EDA...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Cargamos el dataset procesado en el EDA\n",
        "try:\n",
        "    df = pd.read_csv('../data/retail_eda_processed.csv')\n",
        "    print(f\"COMPLETADO: Dataset cargado exitosamente: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: No se encontró el archivo del EDA\")\n",
        "    print(\"   Ejecute primero el notebook EDA.ipynb\")\n",
        "    raise\n",
        "\n",
        "# Cargamos el resumen de hallazgos del EDA\n",
        "try:\n",
        "    with open('../data/eda_summary.pkl', 'rb') as f:\n",
        "        hallazgosEDA = pickle.load(f)\n",
        "        print(f\"COMPLETADO: Resumen de EDA cargado exitosamente\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ADVERTENCIA: No se encontró el resumen del EDA\")\n",
        "    hallazgosEDA = {}\n",
        "\n",
        "# Convertimos la columna de fecha a datetime si no está ya convertida\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Verificamos la integridad de los datos\n",
        "print(f\"\\nVERIFICACIÓN DE INTEGRIDAD:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"• Dimensiones del dataset: {df.shape}\")\n",
        "print(f\"• Período de datos: {df['date'].min().strftime('%Y-%m-%d')} a {df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"• Valores faltantes: {df.isnull().sum().sum()}\")\n",
        "print(f\"• Clientes únicos: {df['customer_id'].nunique()}\")\n",
        "\n",
        "# Verificamos variables creadas en el EDA\n",
        "variablesEsperadas = ['year', 'month', 'day', 'dayOfWeek', 'quarter', 'rangoEtario']\n",
        "variablesPresentes = [var for var in variablesEsperadas if var in df.columns]\n",
        "print(f\"• Variables del EDA presentes: {len(variablesPresentes)}/{len(variablesEsperadas)}\")\n",
        "\n",
        "if len(variablesPresentes) == len(variablesEsperadas):\n",
        "    print(\"COMPLETADO: Todas las variables del EDA están presentes\")\n",
        "else:\n",
        "    print(f\"FALTANTE: Variables faltantes: {set(variablesEsperadas) - set(variablesPresentes)}\")\n",
        "\n",
        "# Mostramos información básica del dataset\n",
        "print(f\"\\nINFORMACIÓN DEL DATASET:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Columnas disponibles: {list(df.columns)}\")\n",
        "print(f\"\\nPrimeras 3 filas:\")\n",
        "print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## **2. Definición del Problema y Variable Target**\n",
        "\n",
        "### **Problema de Machine Learning**\n",
        "\n",
        "Basándose en los hallazgos del EDA, definimos nuestro problema de clasificación:\n",
        "\n",
        "**Objetivo:** Predecir categorías de ventas (Alto, Medio, Bajo) basándose en características de cliente, producto y temporales.\n",
        "\n",
        "**Justificación:** Esta clasificación permite:\n",
        "- Segmentación automática de transacciones\n",
        "- Identificación de clientes de alto valor\n",
        "- Optimización de estrategias de marketing\n",
        "- Predicción de ingresos por categoría\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEFINICIÓN DEL PROBLEMA DE ML...\n",
            "==================================================\n",
            "✓ Variable target creada: 'categoriaVentas'\n",
            "✓ Features numéricas: 7 variables\n",
            "✓ Features categóricas: 3 variables\n",
            "\n",
            "DISTRIBUCIÓN DE LA VARIABLE TARGET:\n",
            "----------------------------------------\n",
            "categoriaVentas\n",
            "Medio    352\n",
            "Bajo     349\n",
            "Alto     299\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balance de clases: 84.9% equilibrio\n",
            "\n",
            "DIMENSIONES PARA ML:\n",
            "• X (features): (1000, 10)\n",
            "• y (target): (1000,)\n",
            "✓ Problema de ML definido correctamente\n"
          ]
        }
      ],
      "source": [
        "# Creación de variable target y selección de features\n",
        "\n",
        "print(\"DEFINICIÓN DEL PROBLEMA DE ML...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Creamos la variable target basada en cuartiles del monto total\n",
        "df['categoriaVentas'] = pd.qcut(df['total_amount'], q=3, labels=['Bajo', 'Medio', 'Alto'])\n",
        "\n",
        "# Seleccionamos features para el modelo\n",
        "featuresNumericas = ['age', 'quantity', 'price_per_unit', 'month', 'day', 'dayOfWeek', 'quarter']\n",
        "featuresCategoricas = ['gender', 'product_category', 'rangoEtario']\n",
        "\n",
        "print(f\"✓ Variable target creada: 'categoriaVentas'\")\n",
        "print(f\"✓ Features numéricas: {len(featuresNumericas)} variables\")\n",
        "print(f\"✓ Features categóricas: {len(featuresCategoricas)} variables\")\n",
        "\n",
        "# Verificamos la distribución de la variable target\n",
        "print(f\"\\nDISTRIBUCIÓN DE LA VARIABLE TARGET:\")\n",
        "print(\"-\" * 40)\n",
        "distribucionTarget = df['categoriaVentas'].value_counts()\n",
        "print(distribucionTarget)\n",
        "print(f\"\\nBalance de clases: {(distribucionTarget.min() / distribucionTarget.max() * 100):.1f}% equilibrio\")\n",
        "\n",
        "# Preparamos datos para transformación\n",
        "X = df[featuresNumericas + featuresCategoricas].copy()\n",
        "y = df['categoriaVentas'].copy()\n",
        "\n",
        "print(f\"\\nDIMENSIONES PARA ML:\")\n",
        "print(f\"• X (features): {X.shape}\")\n",
        "print(f\"• y (target): {y.shape}\")\n",
        "\n",
        "# Guardamos la información de features para el próximo notebook\n",
        "featuresInfo = {\n",
        "    'numericas': featuresNumericas,\n",
        "    'categoricas': featuresCategoricas,\n",
        "    'target': 'categoriaVentas',\n",
        "    'target_labels': ['Bajo', 'Medio', 'Alto']\n",
        "}\n",
        "\n",
        "print(f\"✓ Problema de ML definido correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3. Implementación de ColumnTransformer y Pipeline**\n",
        "\n",
        "### **ColumnTransformer para Transformaciones Específicas**\n",
        "\n",
        "Utilizamos ColumnTransformer para aplicar transformaciones específicas a diferentes tipos de columnas de manera automatizada y eficiente.\n",
        "\n",
        "### **Pipeline para Automatización**\n",
        "\n",
        "Creamos un pipeline completo que automatiza todo el preprocesamiento y asegura la reproducibilidad del proceso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMPLEMENTANDO COLUMNTRANSFORMER Y PIPELINE...\n",
            "============================================================\n",
            "✓ ColumnTransformer configurado:\n",
            "  • Transformador numérico: SimpleImputer + StandardScaler\n",
            "  • Transformador categórico: SimpleImputer + OneHotEncoder\n",
            "  • Variables numéricas: ['age', 'quantity', 'price_per_unit', 'month', 'day', 'dayOfWeek', 'quarter']\n",
            "  • Variables categóricas: ['gender', 'product_category', 'rangoEtario']\n",
            "✓ Pipeline de preprocesamiento creado\n",
            "✓ Target encoder configurado\n",
            "  • Clases: ['Alto' 'Bajo' 'Medio']\n",
            "✓ División estratificada completada:\n",
            "  • Train set: (800, 10)\n",
            "  • Test set: (200, 10)\n",
            "  • Distribución de clases balanceada\n",
            "✓ Pipeline aplicado exitosamente:\n",
            "  • Train procesado: (800, 12)\n",
            "  • Test procesado: (200, 12)\n",
            "✓ Feature names generados: 12 features totales\n",
            "\n",
            "VERIFICACIÓN DE CALIDAD DE TRANSFORMACIONES:\n",
            "--------------------------------------------------\n",
            "• Sin valores faltantes en train: True\n",
            "• Sin valores faltantes en test: True\n",
            "• Media de features numéricas ≈ 0: True\n",
            "• Std de features numéricas ≈ 1: True\n",
            "• Features categóricas binarias: True\n",
            "\n",
            "PREPROCESAMIENTO CON PIPELINE COMPLETADO\n",
            "============================================================\n",
            "✓ Archivo principal: '../data/datos_ml_procesados.pkl'\n",
            "✓ Pipeline guardado: '../data/preprocessed/preprocessor.joblib'\n",
            "✓ Arrays procesados: '../data/preprocessed/*.npy'\n",
            "✓ Información detallada: '../data/preprocessed/preprocessing_info.pkl'\n",
            "\n",
            "LISTO PARA BENCHMARKING\n",
            "   Siguiente paso: ejecutar 'Benchmarking.ipynb'\n"
          ]
        }
      ],
      "source": [
        "# Implementación de ColumnTransformer y Pipeline para preprocesamiento automatizado\n",
        "\n",
        "print(\"IMPLEMENTANDO COLUMNTRANSFORMER Y PIPELINE...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Definimos los transformadores para cada tipo de columna\n",
        "transformadorNumerico = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "transformadorCategorico = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Creamos el ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformadorNumerico, featuresNumericas),\n",
        "        ('cat', transformadorCategorico, featuresCategoricas)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "print(f\"✓ ColumnTransformer configurado:\")\n",
        "print(f\"  • Transformador numérico: SimpleImputer + StandardScaler\")\n",
        "print(f\"  • Transformador categórico: SimpleImputer + OneHotEncoder\")\n",
        "print(f\"  • Variables numéricas: {featuresNumericas}\")\n",
        "print(f\"  • Variables categóricas: {featuresCategoricas}\")\n",
        "\n",
        "# Creamos pipeline completo de preprocesamiento\n",
        "pipeline_preprocesamiento = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "print(f\"✓ Pipeline de preprocesamiento creado\")\n",
        "\n",
        "# Encoding de la variable target\n",
        "targetEncoder = LabelEncoder()\n",
        "y_encoded = targetEncoder.fit_transform(y)\n",
        "\n",
        "print(f\"✓ Target encoder configurado\")\n",
        "print(f\"  • Clases: {targetEncoder.classes_}\")\n",
        "\n",
        "# División estratificada de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"✓ División estratificada completada:\")\n",
        "print(f\"  • Train set: {X_train.shape}\")\n",
        "print(f\"  • Test set: {X_test.shape}\")\n",
        "print(f\"  • Distribución de clases balanceada\")\n",
        "\n",
        "# Aplicamos el pipeline de preprocesamiento\n",
        "X_train_processed = pipeline_preprocesamiento.fit_transform(X_train)\n",
        "X_test_processed = pipeline_preprocesamiento.transform(X_test)\n",
        "\n",
        "print(f\"✓ Pipeline aplicado exitosamente:\")\n",
        "print(f\"  • Train procesado: {X_train_processed.shape}\")\n",
        "print(f\"  • Test procesado: {X_test_processed.shape}\")\n",
        "\n",
        "# Obtenemos los nombres de las features después de la transformación\n",
        "feature_names_out = []\n",
        "\n",
        "# Features numéricas mantienen sus nombres\n",
        "feature_names_out.extend(featuresNumericas)\n",
        "\n",
        "# Features categóricas se expanden con OneHotEncoder\n",
        "encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
        "if hasattr(encoder, 'get_feature_names_out'):\n",
        "    cat_features = encoder.get_feature_names_out(featuresCategoricas)\n",
        "    feature_names_out.extend(cat_features)\n",
        "else:\n",
        "    # Fallback para versiones anteriores de sklearn\n",
        "    for i, cat in enumerate(featuresCategoricas):\n",
        "        n_categories = len(encoder.categories_[i]) - 1  # -1 por drop='first'\n",
        "        feature_names_out.extend([f\"{cat}_{j}\" for j in range(n_categories)])\n",
        "\n",
        "print(f\"✓ Feature names generados: {len(feature_names_out)} features totales\")\n",
        "\n",
        "# Verificamos la calidad de las transformaciones\n",
        "print(f\"\\nVERIFICACIÓN DE CALIDAD DE TRANSFORMACIONES:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"• Sin valores faltantes en train: {not np.isnan(X_train_processed).any()}\")\n",
        "print(f\"• Sin valores faltantes en test: {not np.isnan(X_test_processed).any()}\")\n",
        "print(f\"• Media de features numéricas ≈ 0: {np.abs(X_train_processed[:, :len(featuresNumericas)].mean(axis=0)).max() < 0.01}\")\n",
        "print(f\"• Std de features numéricas ≈ 1: {np.abs(X_train_processed[:, :len(featuresNumericas)].std(axis=0) - 1).max() < 0.01}\")\n",
        "print(f\"• Features categóricas binarias: {np.all(np.isin(X_train_processed[:, len(featuresNumericas):], [0, 1]))}\")\n",
        "\n",
        "# Guardamos todos los objetos necesarios para reproducibilidad\n",
        "datosProcesados = {\n",
        "    'X_train': X_train_processed,\n",
        "    'X_test': X_test_processed,\n",
        "    'y_train': y_train,\n",
        "    'y_test': y_test,\n",
        "    'pipeline_preprocesamiento': pipeline_preprocesamiento,\n",
        "    'target_encoder': targetEncoder,\n",
        "    'feature_names': feature_names_out,\n",
        "    'features_info': {\n",
        "        'numericas': featuresNumericas,\n",
        "        'categoricas': featuresCategoricas,\n",
        "        'target': 'categoriaVentas',\n",
        "        'target_labels': targetEncoder.classes_\n",
        "    }\n",
        "}\n",
        "\n",
        "# Guardamos usando joblib para optimizar el almacenamiento\n",
        "joblib.dump(datosProcesados, '../data/datos_ml_procesados.pkl')\n",
        "\n",
        "# Guardamos también componentes individuales para flexibilidad\n",
        "joblib.dump(pipeline_preprocesamiento, '../data/preprocessed/preprocessor.joblib')\n",
        "joblib.dump(targetEncoder, '../data/preprocessed/target_encoder.joblib')\n",
        "\n",
        "# Guardamos los arrays transformados por separado para eficiencia\n",
        "np.save('../data/preprocessed/X_train_transformed.npy', X_train_processed)\n",
        "np.save('../data/preprocessed/X_test_transformed.npy', X_test_processed)\n",
        "np.save('../data/preprocessed/y_train_encoded.npy', y_train)\n",
        "np.save('../data/preprocessed/y_test_encoded.npy', y_test)\n",
        "\n",
        "# Guardamos información de preprocesamiento\n",
        "preprocessing_info = {\n",
        "    'fecha_procesamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'shape_original': X.shape,\n",
        "    'shape_train_processed': X_train_processed.shape,\n",
        "    'shape_test_processed': X_test_processed.shape,\n",
        "    'feature_names': feature_names_out,\n",
        "    'target_classes': targetEncoder.classes_.tolist(),\n",
        "    'transformaciones_aplicadas': {\n",
        "        'numericas': 'SimpleImputer(median) + StandardScaler',\n",
        "        'categoricas': 'SimpleImputer(constant) + OneHotEncoder(drop_first)',\n",
        "        'target': 'LabelEncoder'\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('../data/preprocessed/preprocessing_info.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_info, f)\n",
        "\n",
        "print(f\"\\nPREPROCESAMIENTO CON PIPELINE COMPLETADO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"✓ Archivo principal: '../data/datos_ml_procesados.pkl'\")\n",
        "print(f\"✓ Pipeline guardado: '../data/preprocessed/preprocessor.joblib'\")\n",
        "print(f\"✓ Arrays procesados: '../data/preprocessed/*.npy'\")\n",
        "print(f\"✓ Información detallada: '../data/preprocessed/preprocessing_info.pkl'\")\n",
        "print(f\"\\nLISTO PARA BENCHMARKING\")\n",
        "print(f\"   Siguiente paso: ejecutar 'Benchmarking.ipynb'\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### **Ventajas del Enfoque ColumnTransformer + Pipeline**\n",
        "\n",
        "#### **ColumnTransformer Beneficios:**\n",
        "- **Transformaciones Específicas**: Cada tipo de variable recibe el tratamiento apropiado\n",
        "- **Automatización**: No hay necesidad de transformaciones manuales\n",
        "- **Consistencia**: Mismas transformaciones en train y test\n",
        "- **Escalabilidad**: Fácil agregar nuevos tipos de transformaciones\n",
        "\n",
        "#### **Pipeline Beneficios:**\n",
        "- **Reproducibilidad**: Los pasos están claramente definidos y son repetibles\n",
        "- **Prevención de Data Leakage**: Las transformaciones se aprenden solo del conjunto de entrenamiento\n",
        "- **Simplicidad**: Un solo objeto maneja todo el preprocesamiento\n",
        "- **Integración**: Compatible con todas las herramientas de scikit-learn\n",
        "\n",
        "#### **Transformaciones Aplicadas:**\n",
        "\n",
        "**Variables Numéricas:**\n",
        "- `SimpleImputer(strategy='median')`: Manejo de valores faltantes\n",
        "- `StandardScaler()`: Normalización (media=0, std=1)\n",
        "\n",
        "**Variables Categóricas:**\n",
        "- `SimpleImputer(strategy='constant')`: Manejo de valores faltantes\n",
        "- `OneHotEncoder(drop='first')`: Codificación binaria con eliminación de multicolinealidad\n",
        "\n",
        "#### **Validación de Calidad:**\n",
        "- Sin valores faltantes en datasets procesados\n",
        "- Variables numéricas normalizadas correctamente\n",
        "- Variables categóricas codificadas como binarias\n",
        "- Nombres de features rastreables\n",
        "- Pipeline completo serializado para reutilización\n",
        "\n",
        "#### **Archivos Generados:**\n",
        "- `datos_ml_procesados.pkl`: Dataset completo listo para ML\n",
        "- `preprocessor.joblib`: Pipeline reutilizable para nuevos datos\n",
        "- `*.npy`: Arrays procesados para carga rápida\n",
        "- `preprocessing_info.pkl`: Metadata y documentación\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
